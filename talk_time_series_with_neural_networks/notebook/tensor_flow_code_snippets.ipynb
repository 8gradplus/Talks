{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pandas import DataFrame\n",
    "\n",
    "@dataclass\n",
    "class Data:\n",
    "    train: DataFrame\n",
    "    validation: DataFrame\n",
    "    test: DataFrame    \n",
    "\n",
    "    def map(self, f):\n",
    "        return Data(f(self.train), f(self.validation), f(self.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame\n",
    "\n",
    "class IdentityScaler: \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x.values\n",
    "    \n",
    "    def inverse_transform(self, x):\n",
    "        return x.values\n",
    "    \n",
    "\n",
    "\n",
    "class Scalers:\n",
    "    \n",
    "    def __init__(self, types):\n",
    "        self.types=types\n",
    "        self.scalers = [] \n",
    "        self.indices = {}\n",
    "    \n",
    "    def fit(self, df: DataFrame):\n",
    "        self.assign_indices(df)\n",
    "        for col in df.columns: \n",
    "            if self.types[col] == \"categorical\":\n",
    "                scaler = OrdinalEncoder()\n",
    "                scaler.fit(df[[col]])\n",
    "                self.scalers.append(scaler)\n",
    "            elif self.types[col] == \"continuous\":\n",
    "                scaler = MinMaxScaler()\n",
    "                scaler.fit(df[[col]])\n",
    "                self.scalers.append(scaler)\n",
    "            elif self.types[col] == \"identity\":\n",
    "                scaler = IdentityScaler()\n",
    "                self.scalers.append(scaler)\n",
    "            else: \n",
    "                raise LookupError(f\"could not find type for column '{col}' given columns {df.columns}\")\n",
    "                \n",
    "    def assign_indices(self, df): \n",
    "        for i, col in enumerate(df.columns):\n",
    "            self.indices.update({col: i})\n",
    "    \n",
    "    def transform(self, df: DataFrame):\n",
    "        index = df.index\n",
    "        transformed = {}\n",
    "        for col in df.columns:\n",
    "            i = self.indices[col]\n",
    "            trafo = self.scalers[i].transform(df[[col]]).flatten()\n",
    "            transformed.update({col: trafo})\n",
    "        return pd.DataFrame(transformed, index=index)\n",
    "    \n",
    "    def inverse_transform(self, xs, col:str): \n",
    "        i = self.indices[col]\n",
    "        scaler = self.scalers[i]\n",
    "        return scaler.inverse_transform(xs)\n",
    "    \n",
    "    \n",
    "\n",
    "class GetNumberOfCategories:\n",
    "    \n",
    "    def __init__(self, df: DataFrame):\n",
    "        self.df = df\n",
    "    \n",
    "    def __call__(self, column: str):\n",
    "        return self.df[column].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "\n",
    "class Offsets:\n",
    "    def __init__(self, \n",
    "                 input_width: int, \n",
    "                 label_width: int, \n",
    "                 shift: int=None):\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width \n",
    "        self.shift = shift\n",
    "        if not shift:\n",
    "            self.shift = self.label_width\n",
    "        self.sequence_width = self.input_width + self.shift\n",
    "        \n",
    "\n",
    "class SplitInTime:\n",
    "    \n",
    "    def __init__(self, offsets: Offsets):\n",
    "        self.offsets = offsets\n",
    "    \n",
    "    def __call__(self, t: tf.Tensor):\n",
    "        features = t[: -self.offsets.shift]\n",
    "        labels = t[-self.offsets.label_width :]\n",
    "        return (features, labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_label_indices(df: DataFrame, labels: list):\n",
    "    return [idx for idx, name in enumerate(df.columns) if name in labels]\n",
    "\n",
    "    \n",
    "class PickLabels:\n",
    "    \n",
    "    def __init__(self, label_indices: list):\n",
    "        self.label_indices = label_indices\n",
    "        \n",
    "    def __call__(self, features, labels):\n",
    "        picked_labels = [labels[:, i] for i in self.label_indices]\n",
    "        picked_labels = tf.stack(picked_labels, axis=1)\n",
    "        return (features, picked_labels)\n",
    "    \n",
    "        \n",
    "class BatchWindow:\n",
    "    \n",
    "    def __init__(self, sequence_width: int):\n",
    "        self.sequence_width = sequence_width\n",
    "    \n",
    "    def __call__(self, window: Dataset):\n",
    "        return window.batch(self.sequence_width)\n",
    "        \n",
    "        \n",
    "        \n",
    "class MakeDatasetFromDataFrame:\n",
    "    \n",
    "    def __init__(self, offsets: Offsets, batch_size: int, labels: list):\n",
    "        self.offsets = offsets\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __call__(self, df: DataFrame):\n",
    "        sequence_width = self.offsets.sequence_width\n",
    "        label_indices = get_label_indices(df, self.labels)\n",
    "        return (Dataset.from_tensor_slices(df)\n",
    "                .window(sequence_width, 1, drop_remainder=True)\n",
    "                .flat_map(BatchWindow(sequence_width))\n",
    "                .map(SplitInTime(offsets))\n",
    "                .map(PickLabels(label_indices))\n",
    "                .batch(self.batch_size)\n",
    "                .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "    \"\"\"Embedding layer: each feature is embedded according to type\"\"\"\n",
    "    \n",
    "    def __init__(self, types, hidden_dim, time_steps, name=\"Embeddings\", **kwargs):\n",
    "        super(Embeddings, self).__init__(name=name, **kwargs)\n",
    "        self.types = types\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.time_steps = time_steps\n",
    "        self.embedding_layers = self.setup_layers()\n",
    "        \n",
    "        \n",
    "    def setup_layers(self):\n",
    "        embeddings = []\n",
    "        for feature, data_type in self.types.items(): \n",
    "            if data_type == \"categorical\":\n",
    "                embedding = tf.keras.layers.Embedding(input_dim=get_number_of_categories(feature), \n",
    "                                                      output_dim=self.hidden_dim, \n",
    "                                                      input_length=self.time_steps,\n",
    "                                                      name=feature)\n",
    "            elif data_type == \"continuous\":\n",
    "                embedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=self.hidden_dim), name=feature)\n",
    "            elif data_type == \"identity\":\n",
    "                embedding = Identity(name=feature)\n",
    "            else: \n",
    "                raise LookupError(f\"could not find data type {data_type}\")\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings\n",
    "            \n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # todo: checks\n",
    "        result = []\n",
    "        for feature_idx, data_type in enumerate(self.types.values()):\n",
    "            # keep dims for non_categorical variables.\n",
    "            xs = inputs[..., feature_idx, None] \n",
    "            if data_type == \"categorical\":\n",
    "                xs = inputs[..., feature_idx]\n",
    "            result.append(self.embedding_layers[feature_idx](xs))\n",
    "        return result\n",
    "    \n",
    "    \n",
    "class Concatenation(tf.keras.layers.Layer):\n",
    "    \"\"\"Concatination layer [tensors] -> tensor\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Concatenation\", **kwargs):\n",
    "        super(Concatenation, self).__init__(name=name, **kwargs)\n",
    "    \n",
    "        \n",
    "    def call(self, inputs:list):\n",
    "        return tf.concat(inputs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "types = {\"n_dispatched\": \"continuous\", \n",
    "         \"n_returns_expected\": \"continuous\", \n",
    "         \"bayes_return_probability_mean\": \"continuous\", \n",
    "         \"dominant_product_group\": \"categorical\", \n",
    "         \"day_of_year\": \"continuous\", \n",
    "         \"day_of_week\": \"continuous\",\n",
    "         \"n_returned\": \"identity\"} # target\n",
    "\n",
    "# Load data\n",
    "data = load_data() #type Data\n",
    "\n",
    "# Number of categories for later embedding layer\n",
    "get_number_of_categories = GetNumberOfCategories(data.train)\n",
    "\n",
    "# scaler\n",
    "scalers = Scalers(types)\n",
    "scalers.fit(data.train)\n",
    "\n",
    "# parametrize rolling window\n",
    "offsets = Offsets(20, 20, 1) \n",
    "generate_ds MakeDatasetFromDataFrame(offsets=offsets, batch_size=32, labels=[\"n_returned\"])\n",
    "\n",
    "# apply transformations\n",
    "ds = data.map(scalers.transform).map(generate_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Ar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "def negative_log_likelihood(y, p_y):\n",
    "    \"\"\"Loss function\"\"\"\n",
    "    return -p_y.log_prob(y)\n",
    "\n",
    "\n",
    "class Gaussian(tf.keras.layers.Layer):\n",
    "    \"\"\"Keras Layer for gaussion distribution\"\"\"\n",
    "    \n",
    "    def __init__(self,  name=\"probabilistic_gauss\", **kwargs):\n",
    "        super(GaussProbabilistic, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "    \n",
    "    def call(self, xs):\n",
    "        def distribution(xs):\n",
    "            loc = xs[..., 0, None]\n",
    "            scale = tf.math.softplus(xs[..., 1, None])\n",
    "            return tfp.distributions.Normal(loc=loc, scale=scale)\n",
    "        return tfp.layers.DistributionLambda(distribution, name='normal_dist')(xs)\n",
    "    \n",
    "    \n",
    "    \n",
    "class NegativeBinomial(tf.keras.layers.Layer):\n",
    "    def __init__(self,  name=\"probabilistic_negative_binomial\", **kwargs):\n",
    "        super(NegativeBinomial, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "    def call(self, xs):\n",
    "        def distribution(xs):\n",
    "            total_count = tf.math.softplus(xs[..., 0, None])\n",
    "            logits = xs[..., 1, None]\n",
    "            return tfp.distributions.NegativeBinomial(total_count=total_count, logits=logits)\n",
    "        return tfp.layers.DistributionLambda(distribution, name='negative_binomial_dist')(xs)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class DeepAr(keras.Model):\n",
    "    \n",
    "    def __init__(self, hidden_dim, out_dim=1, name=\"deepar\", **kwargs):\n",
    "        super(DeepArTfp, self).__init__(name=name, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.lstm_cell = keras.layers.LSTMCell(hidden_dim)\n",
    "        self.distribution_params = keras.layers.Dense(2, activation=\"linear\")\n",
    "        self.distribution = NegativeBinomial()\n",
    "        \n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        rnn = keras.layers.RNN(self.lstm_cell, return_state=True, return_sequences=True)\n",
    "        xs, h, c = rnn(inputs)\n",
    "        return xs, h, c\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        xs, _, _ = self.encode(inputs)\n",
    "        xs = TimeDistributed(self.distribution_params)(xs)\n",
    "        xs = self.distribution(xs)\n",
    "        return xs \n",
    "    \n",
    "    def sample_trace(self, inputs, warmup_period):\n",
    "        \"\"\"Sample single trace from likelihood function\"\"\"\n",
    "        predictions = []\n",
    "        warmup = inputs[:, :warmup_period, :] \n",
    "        _, *states = self.encode(warmup)\n",
    "        prediction = self.distribution(self.distribution_params(states[0])).sample(1)[0, ...]\n",
    "        predictions.append(prediction) # (batch, target)\n",
    "        t_final = inputs.shape[1]\n",
    "        for t in range(warmup_period + 1, t_final):\n",
    "            covariates = tf.cast(inputs[:, t, :-1], prediction.dtype)\n",
    "            xs = tf.concat([covariates, prediction], axis=1)\n",
    "            ys, states = self.lstm_cell(xs, states=states)\n",
    "            prediction = self.distribution(self.distribution_params(ys)).sample(1)[0, ...]\n",
    "            predictions.append(prediction)\n",
    "        predictions = tf.stack(predictions, axis=0) #(time, batch, target)\n",
    "        predictions = tf.transpose(predictions, [1,0,2]) #(batch, time, target)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "deep_ar = DeepAr(60)\n",
    "mse = tf.keras.metrics.MeanSquaredError()\n",
    "mae = tf.keras.metrics.MeanAbsoluteError()\n",
    "metrics = [mse, mae]\n",
    "deep_ar.compile(optimizer=optimizer, loss=negative_log_likelihood, metrics=metrics)\n",
    "deep_ar.fit(ds.train, validation_data=tds.validation, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
